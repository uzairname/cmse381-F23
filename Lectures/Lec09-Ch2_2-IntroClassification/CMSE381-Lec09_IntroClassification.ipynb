{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfd68f5",
   "metadata": {},
   "source": [
    "# Lab: Intro to Classification\n",
    "## CMSE 381 - Fall 2023\n",
    "## Sep 18, 2023\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2bdd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b964f8",
   "metadata": {},
   "source": [
    "## Reading in the chicken or egg data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a53af",
   "metadata": {},
   "source": [
    "In this lab, we are going to try out the KNN classification described in class. First, we're going to load up our data. This data is 100% made up by Dr. Munch. Based on two inputs $x_1$ and $x_2$, we get to predict whether we have a `chicken` or an `egg`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ceb8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Chick_df = pd.read_csv('../../DataSets/ChickenEgg.csv')\n",
    "Chick_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9763db94",
   "metadata": {},
   "source": [
    "The first step is always to do some data exploration. \n",
    "\n",
    "&#9989; **<font color=red>Do this:</font>** Draw a scatter plot of your data with the relevant labels. \n",
    "\n",
    "*Hint: There are many ways to do this, but `sns.scatterplot` often works the fastest, and if you set `hue` and/or `style` to the `Label` column, you should get the labels drawn as color and/or symbol.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5177b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6115fff9",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Based on your scatter plot, what you expect a KNN classifer with $k=1$ to predict for a data point at $(-4,4)$? What about at $(6,0)$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc2723",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0561116",
   "metadata": {},
   "source": [
    "## KNN with `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e25e2d9",
   "metadata": {},
   "source": [
    "Ok, but let's be honest, no one is going to do this by hand everytime. Let's make `sklearn` do it for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea1279",
   "metadata": {},
   "source": [
    "First, we're going to split our data into two pieces. The $X$ input variables, and the $Y$ output variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcc24b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Chick_df.drop('Label', axis =1)\n",
    "y = Chick_df['Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3427c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b0f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce1677",
   "metadata": {},
   "source": [
    "Then the following code trains our classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c56001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed94b6cf",
   "metadata": {},
   "source": [
    "...and this command provides a prediction for an input of  $(-2,3)$ input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ff516",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict([[-2,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae744f8e",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** What does your classifier predict for $(-4,4)$? What about at $(6,0)$? Are these the same that you guessed above? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc2d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac89745",
   "metadata": {},
   "source": [
    "This predict function is pretty powerful. If I want to get all predictions for all of my inputs, I can just pass my $X$ dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dd6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e61c3e",
   "metadata": {},
   "source": [
    "Remember what all the actual labels were? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c6ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c338d99",
   "metadata": {},
   "source": [
    "Now, I could sit here and go through one by one to see if they have the same predicted value as the label to decide on my accuracy. But, as usual, `sklearn` comes to the rescue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831dea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(knn.predict(X),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10517295",
   "metadata": {},
   "source": [
    "## Train test splits \n",
    "\n",
    "Ok, so you got 100% accuracy! You're done, right???? \n",
    "\n",
    "**<font color=red>Wrong!</font>**\n",
    "\n",
    "We know better than to report our training accuracy as our actual accuracy! So, let's set up some basic train/test splits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc5ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca9c3be",
   "metadata": {},
   "source": [
    "In this case, the `X_train` data frame has the inputs we'll use for training, and the `y_train` has the outputs for those same data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a723c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5fb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bda846",
   "metadata": {},
   "source": [
    "The `X_test` data frame has the inputs we'll use for test, and the `y_test` has the outputs for those same data points. We don't get to touch these until after the training is all done! Otherwise we are data-snooping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded2968",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "X_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4097eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.shape)\n",
    "y_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a170d0a",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Build a KNN classifier using $k=3$ neighbors, and train it on your `X_train` and `y_train` data. Call it `knn` like before. \n",
    "\n",
    "- What is your training accuracy? \n",
    "- What is your testing accuracy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0010be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab10a3d",
   "metadata": {},
   "source": [
    "I want to show you one more nice command in here. Remember that KNNs work by returning the label of the most frequently seen label among the $k$ numbers, but that doesn't mean every neighbor had that label. \n",
    "\n",
    "The `predict_proba` function will tell you the percentage of each that was seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1127ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903eff51",
   "metadata": {},
   "source": [
    "## Bayes classifier \n",
    "\n",
    "Now, because I generated our data, we can take a look at how our results match up with the Bayes classifier. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell, we just need to generate some inputs \n",
    "# to be able to draw figures in a moment. \n",
    "t = np.linspace(-7,7,28)\n",
    "X_mesh,Y_mesh = np.meshgrid(t,t)\n",
    "X_mesh_flat = X_mesh.flatten()\n",
    "Y_mesh_flat = Y_mesh.flatten()\n",
    "test_all_df = pd.DataFrame({'x1':X_mesh_flat,'x2':Y_mesh_flat})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf1fd25",
   "metadata": {},
   "source": [
    "First, I am going to set up some code that will figure out what your model, named `knn` above hopfully, will predict for a grid of numbers covering our $[-7,7] \\times [-7,7]$ box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088dc7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all = knn.predict(test_all_df)\n",
    "def to_int(chickegg):\n",
    "    if chickegg == 'egg':\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "pred_all = np.array([to_int(x) for x in pred_all])\n",
    "pred_all.shape\n",
    "pred_all = pred_all.reshape([28,28])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf9096b",
   "metadata": {},
   "source": [
    "Then this will plot these predictions on our 2D plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38863195",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(X_mesh,Y_mesh,pred_all)\n",
    "\n",
    "cbar = plt.colorbar(ticks=[-1, 1])\n",
    "cbar.ax.set_yticklabels(['egg','chicken'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9422cb5b",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Q:</font>** Using this plot, what is your model going to predict for a data point at $(-4,6)$? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757bfc21",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc00b0",
   "metadata": {},
   "source": [
    "Note that since I generated this data, I know where the line was between the two regions used to generate it. In my case, I happened to use the function \n",
    "$$\n",
    "f(x_1,x_2) = (x_2)^3 - x_2 -x_1\n",
    "$$\n",
    "and labeled a new data point based on whether $f(x_1,x_2) + \\varepsilon$ was positive or negative. \n",
    "\n",
    "Below, you can see $f(x_1,x_2)$ (in this case, the Bayes classifier) drawn on top of your model's predictions. How similar did you get? \n",
    "\n",
    "&#9989; **<font color=red>Q:</font>** Where are the regions that your model predicts something different than the Bayes classifier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aa1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command tries to draw a line between the two regions \n",
    "plt.pcolor(X_mesh,Y_mesh,pred_all)\n",
    "\n",
    "cbar = plt.colorbar(ticks=[-1, 1])\n",
    "cbar.ax.set_yticklabels(['egg','chicken'])\n",
    "\n",
    "# This draws the line I used to generate the data \n",
    "ty = np.linspace(-2.1,2.1,100)\n",
    "tx = ty**3 - ty\n",
    "plt.plot(tx,ty, color = 'grey')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860aee34",
   "metadata": {},
   "source": [
    "## Messing with $k$\n",
    "\n",
    "Finally, we're going to generate plots like Fig 2.17 in the book, where we look at the training and testing errors vs the flexibility (in this case, $1/k$) of the model used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f2c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our choices of $K$\n",
    "# Note that the graph will use $1/k$ for flexibility\n",
    "Kinv = np.array([0.04, 0.05,0.2,0.5,1])\n",
    "Ks = 1/Kinv\n",
    "Ks = Ks.astype(int)\n",
    "print(Ks)\n",
    "\n",
    "Accuracies = []\n",
    "TrainAccuracies = []\n",
    "\n",
    "\n",
    "# I'm going to run this model for all my choices of k\n",
    "for k in Ks:\n",
    "    thisruntest = []\n",
    "    thisruntrain = []\n",
    "    \n",
    "    # For each choice of k, i'll do this 10 times and average\n",
    "    for runNum in range(10):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train,y_train)\n",
    "        \n",
    "        # Figure out my training error rate \n",
    "        acctrain = accuracy_score(knn.predict(X_train),y_train)\n",
    "        thisruntrain.append(1-acctrain)\n",
    "        \n",
    "        # Figure out my testing error rate\n",
    "        acctest = accuracy_score(knn.predict(X_test),y_test)\n",
    "        thisruntest.append(1-acctest)\n",
    "    \n",
    "    # Keep the average over the 10 runs\n",
    "    TrainAccuracies.append(np.average(thisruntrain))\n",
    "    Accuracies.append(np.average(thisruntest))\n",
    "\n",
    "    \n",
    "# Plot train and test with x-axis on a log scale\n",
    "plt.semilogx(1/Ks,TrainAccuracies, label = 'Train')\n",
    "plt.semilogx(1/Ks,Accuracies, label = 'Test')\n",
    "plt.xlabel('1/k')\n",
    "plt.ylabel('Error rate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ed8c2",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Q:</font>** Based on your graph above\n",
    "- What do you notice about the shape of the train and test error plots? \n",
    "- What would you choose for $k$ based on this data? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f79113",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "-----\n",
    "### Congratulations, we're done!\n",
    "Written by Dr. Liz Munch, Michigan State University\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877d319",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb8354f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

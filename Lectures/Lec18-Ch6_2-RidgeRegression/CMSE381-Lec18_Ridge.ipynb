{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfd68f5",
   "metadata": {},
   "source": [
    "# Lec 18: Ridge Regression\n",
    "## CMSE 381 - Fall 2023\n",
    "## Oct 16, 2023\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90aa0b0",
   "metadata": {},
   "source": [
    "In this module we are going to test out the ridge regression method we discussed in class from Chapter 6.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everyone's favorite standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# ML imports we've used previously\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Fix for deprecation warnings, but we should fix so this isn't here\n",
    "# import warnings \n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9a257",
   "metadata": {},
   "source": [
    "# Loading in the data\n",
    "\n",
    "Ok, here we go, let's play with a baseball data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a59a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters_df = pd.read_csv('../../DataSets/Hitters.csv')\n",
    "hitters_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3549cda",
   "metadata": {},
   "source": [
    "Annoyingly enough we have some missing values in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9032bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of null values:\", hitters_df[\"Salary\"].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a40baef",
   "metadata": {},
   "source": [
    "So let's go clean those up....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dimensions of the original Hitters data (322 rows x 20 columns)\n",
    "print(\"Dimensions of original data:\", hitters_df.shape)\n",
    "\n",
    "# Drop any rows the contain missing values, along with the player names\n",
    "hitters_df = hitters_df.dropna().drop('Player', axis=1)\n",
    "\n",
    "# Print the dimensions of the modified Hitters data (263 rows x 20 columns)\n",
    "print(\"Dimensions of modified data:\", hitters_df.shape)\n",
    "\n",
    "# One last check: should return 0\n",
    "print(\"Number of null values:\", hitters_df[\"Salary\"].isnull().sum())\n",
    "\n",
    "hitters_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a060d0",
   "metadata": {},
   "source": [
    "And finally, we can replace our categorical variables with dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters_df = pd.get_dummies(hitters_df, drop_first = True)\n",
    "hitters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = hitters_df.Salary\n",
    "\n",
    "# Drop the column with the independent variable (Salary)\n",
    "X = hitters_df.drop(['Salary'], axis = 1).astype('float64')\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3656818c",
   "metadata": {},
   "source": [
    "## Normalizing the data \n",
    "\n",
    "Our first job is to normalize the data. Right now, all our columns have different standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7660b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d801e4a",
   "metadata": {},
   "source": [
    "We'll do it on all the columns, but I'll draw pictures just with the `Hits` column for ease of visualization. Here's that the distribution of that particular column is before doing any normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea209d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(X.Hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f3daa2",
   "metadata": {},
   "source": [
    "I'm going to take all my columns and normalize them so that each column has standard deviation 1. I'll do this using the `Normalizer` command for reasons that will be helpful later when I want to fix this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995283c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we set up the transformer to figure out what has to happen \n",
    "# to each column to get out normalized data\n",
    "transformer = StandardScaler().fit(X)\n",
    "\n",
    "# This command tells the transformer I want a pandas data frame back\n",
    "# otherwise it will hand me a numpy array.\n",
    "transformer.set_output(transform = 'pandas')\n",
    "\n",
    "#Then I actually use the transform bit to do the action on all the columns. \n",
    "X_norm = transformer.transform(X)\n",
    "\n",
    "X_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd91f2",
   "metadata": {},
   "source": [
    "Woohoo, I have normalized data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a40800",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1a144",
   "metadata": {},
   "source": [
    "Notice that the shape of the `Hits` histogram is the same, we've just scaled the $x$-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad309ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(X_norm.Hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f83fd5",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "In class, we learned that doing ridge regression means that we try to find the best model accoding to the score\n",
    "$$\n",
    "RSS + \\lambda \\sum_{i} \\beta_i^2.\n",
    "$$\n",
    "The good news is that `scikitlearn` has a built in `Ridge` function.  \n",
    "\n",
    "- [Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "- [User guide](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a7810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21cc7f7",
   "metadata": {},
   "source": [
    "The bad (ok, not honestly that bad) news is that they call their $\\lambda$ parameter $\\alpha$. So we're just going to minimize \n",
    "$$\n",
    "RSS + \\alpha \\sum_{i} \\beta_i^2.\n",
    "$$\n",
    "instead. So if I pick an $\\alpha$ value, I can do ridge regression as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e548cf0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = 1 #<------ this is me picking an alpha value\n",
    "ridge = Ridge(alpha = a) \n",
    "\n",
    "# normalize the input\n",
    "transformer = StandardScaler().fit(X)\n",
    "transformer.set_output(transform = 'pandas')\n",
    "X_norm = transformer.transform(X)\n",
    "\n",
    "# Fit the regression\n",
    "ridge.fit(X_norm, y)\n",
    "\n",
    "# Get all the coefficients\n",
    "print('intercept:', ridge.intercept_)\n",
    "print('\\n')\n",
    "print(pd.Series(ridge.coef_, index = X_norm.columns))\n",
    "print('\\nTraining MSE:',mean_squared_error(y,ridge.predict(X_norm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56205715",
   "metadata": {},
   "source": [
    "That is a bit annoying in terms of how much stuff I have to do, so here's some code that does the exact same thing (note that your score and coefficients learned are exactly the same as above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "a = 1 #<------ this is me picking an alpha value\n",
    "\n",
    "\n",
    "# The make_pipeline command takes care of the normalization and the \n",
    "# Ridge regression for you. Note that I am passing in the un-normalized\n",
    "# matrix X everywhere in here, since the normalization happens internally.\n",
    "model = make_pipeline(StandardScaler(), Ridge(alpha = a))\n",
    "model.fit(X, y)\n",
    " \n",
    "# Get all the coefficients. Notice that in order to get \n",
    "# them out of the ridge portion, we have to ask the pipeline \n",
    "# for the specific bit we want with the model.named_steps['ridge']\n",
    "# in place of just ridge from above.\n",
    "print('intercept:', model.named_steps['ridge'].intercept_)\n",
    "print('\\n')\n",
    "print(pd.Series(model.named_steps['ridge'].coef_, index = X.columns))\n",
    "print('\\nTraining MSE:',mean_squared_error(y,model.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d25cce",
   "metadata": {},
   "source": [
    "Of course, that was just me picking a random $\\alpha$ out of a hat so there's no reason to trust that it's a good one. I could sit here all day and move that $\\alpha$ around to see what's going on, but why do that, when I can make a for loop!\n",
    "\n",
    "Here's a pile of $\\alpha$s for us to test on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a875cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(4,-2,100)*0.5\n",
    "alphas = np.append(alphas,0)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7d033",
   "metadata": {},
   "source": [
    "First off, let's take a look at how the coefficients learned change for various choices of $\\alpha$. \n",
    "\n",
    "Associated with each alpha value is a vector of ridge regression coefficients, which we'll store in a matrix coefs. In this case, it is a  19×100  matrix, with 19 rows (one for each predictor) and 100 columns (one for each value of alpha). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "ridge = Ridge()\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    model = make_pipeline(StandardScaler(), Ridge(alpha = a))\n",
    "    model.fit(X, y)\n",
    "    coefs.append(model.named_steps['ridge'].coef_)\n",
    "    \n",
    "\n",
    "coefs = pd.DataFrame(coefs,columns = X_norm.columns)\n",
    "coefs.head()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9f5b3",
   "metadata": {},
   "source": [
    "Let's say I just want to look at the coefficient for the `Hits` column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d66723",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas,coefs.Hits, label = 'Hits')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97909990",
   "metadata": {},
   "source": [
    "Or just for the `Runs` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e516a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas,coefs.Runs, label = 'Runs')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad8f06",
   "metadata": {},
   "source": [
    "But that's pretty annoying, so instead we look at all of the coefficients at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ae261",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, coefs, label = coefs.columns)\n",
    "plt.xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('coefficients')\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1.1,1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0100e9",
   "metadata": {},
   "source": [
    "\n",
    "&#9989; **<font color=red>Q:</font>** There are two variables that have higher magnitude than the rest for low $\\alpha$s (read: are either very large positive or very large negative). Which two are they from the data set? Which is which?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ad591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1269c032",
   "metadata": {},
   "source": [
    "## Train/test split version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3012f155",
   "metadata": {},
   "source": [
    "Now we can start setting up the usual train/test splits to have at least a starting idea of how the testing error is going. The `random_state=1` bit just makes it so that everyone should get the same random split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d835cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee551fba",
   "metadata": {},
   "source": [
    "\n",
    "&#9989; **<font color=red>Do this:</font>** Train a model using ridge regression with $\\alpha = 4$. What is the MSE of your model on the testing set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51f78b6",
   "metadata": {},
   "source": [
    "\n",
    "&#9989; **<font color=red>Do this:</font>** Ha ha nah, you can do better than that.  Lets try all our alphas and take a look at the testing MSE to make a better decision about what $\\alpha$ we might want. Modify the code below to plot your testing MSE for all the alphas. What $\\alpha$ should we use to train the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify your code from above and add it in the for loop to plot the testing MSE\n",
    "\n",
    "ridge = Ridge()\n",
    "errors = []\n",
    "\n",
    "for a in alphas:\n",
    "    # ==== Your code goes in here ==== #\n",
    "    errors.append(17) #<----- random number in here so that the code runs before you fix it\n",
    "\n",
    "#---plotting----\n",
    "\n",
    "plt.plot(alphas,errors)\n",
    "plt.title('Testing MSE')\n",
    "ax=plt.gca()\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e0478",
   "metadata": {},
   "source": [
    "## RidgeCV\n",
    "\n",
    "Whelp, your meanie professor didn't tell you that there's actually a built in function to do this for you (sorry-not-sorry). Aren't you glad you didn't read ahead?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f8331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d1510b",
   "metadata": {},
   "source": [
    "- [Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)\n",
    "- [User Guide](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)\n",
    "\n",
    "Basically, `RidgeCV` runs LOOCV (unless you tell it otherwise, see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)) on all the $\\alpha$ values you specify on an input array, and tells you the best $\\alpha$ given that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c751b",
   "metadata": {},
   "source": [
    "First, I am going to normalize. I'm being careful to not just normalize all of $X$ and then pass this matrix on since this is an example of [data leakage](https://scikit-learn.org/stable/common_pitfalls.html#data-leakage-during-pre-processing). In that version, we are using some amount of information about the testing data (in this case it is contributing to the normalization process) to affect something in our training process. So here's what we do instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c6fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# To make sure my normalization isn't snooping, I fit the transformer only \n",
    "# on the training set \n",
    "transformer = StandardScaler().fit(X_train)\n",
    "transformer.set_output(transform = 'pandas')\n",
    "X_train_norm = transformer.transform(X_train)\n",
    "\n",
    "# but in order for my output results to make sense, I have to apply the same \n",
    "# transformation to the testing set. \n",
    "X_test_norm = transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b437b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to drop that 0 from the alphas because it makes \n",
    "# RidgeCV cranky\n",
    "alphas = alphas[:-1]\n",
    "\n",
    "\n",
    "ridgecv = RidgeCV(alphas = alphas, \n",
    "                  scoring = 'neg_mean_squared_error', \n",
    "                  )\n",
    "ridgecv.fit(X_train_norm, y_train)\n",
    "\n",
    "\n",
    "\n",
    "print('alpha chosen is', ridgecv.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7759291",
   "metadata": {},
   "source": [
    "I can predict my values on the test set directly from the `ridgecv` model we just built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8577eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ridgecv.predict(X_test_norm)\n",
    "mean_squared_error(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe0dfc5",
   "metadata": {},
   "source": [
    "This is exactly the same result as if I went and retrained my model using the chosen $\\alpha$ using `Ridge`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad65f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha = ridgecv.alpha_)\n",
    "ridge.fit(X_train_norm, y_train)\n",
    "mean_squared_error(y_test, ridge.predict(X_test_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d5384c",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Why did we get a different best choice of $\\alpha$ than we found in the previous section? \n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f79113",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "-----\n",
    "### Congratulations, we're done!\n",
    "Written by Dr. Liz Munch, Michigan State University\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb8354f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

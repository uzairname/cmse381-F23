{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfd68f5",
   "metadata": {},
   "source": [
    "# More K-Fold CV \n",
    "## CMSE 381 - Fall 2023\n",
    "## Oct 6,  2023. Lecture 14\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everyone's favorite standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006fbd25",
   "metadata": {},
   "source": [
    "# 1. Setting $k$-fold up on a slightly more complicated data set. \n",
    "\n",
    "Ok, let's see how we can use $k$-fold CV for determining hyperparameters. Below, we're going to generate a data set that is clearly non-linear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa8b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed so everyone has the same numbers\n",
    "np.random.seed(42)\n",
    "\n",
    "def f(t, m1 = -7,m2 = 5, m3 = -.8, b = 6):\n",
    "    return m3 * t**3 + m2*t**2 + m1*t+b\n",
    "\n",
    "n = 300\n",
    "X_toy = np.random.uniform(-1,5,n)\n",
    "y_toy = f(X_toy) + np.random.normal(0,2,n)\n",
    "\n",
    "plt.scatter(X_toy,y_toy)\n",
    "\n",
    "\n",
    "# Doing this so the plot isn't ugly\n",
    "X_plot = X_toy.copy()\n",
    "X_plot.sort()\n",
    "plt.plot(X_plot,f(X_plot),c = 'red')\n",
    "\n",
    "\n",
    "X_toy = X_toy.reshape(-1,1)\n",
    "y_toy = y_toy.reshape(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c63597",
   "metadata": {},
   "source": [
    "To do this, we are going to set up a polynomial model. For a fixed degree $p$, we want to use the model \n",
    "$$y = \\beta_0 + \\beta_1 X+ \\beta_2 X^2+ \\cdots+ \\beta_p X^p$$\n",
    "\n",
    "Before messing with this on our big data set, let's see how we can trick linear regression into doing our work for us. \n",
    "Take a look at my silly input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a22aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(10).reshape(-1,1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b829e503",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Given this input data, what is each column in the following matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fa1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5273e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 4\n",
    "poly = PolynomialFeatures(p)\n",
    "X_powers = poly.fit_transform(X)\n",
    "\n",
    "X_powers\n",
    "\n",
    "#This version might be easier to read, uncomment if it helps\n",
    "# X_powers.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a73b29",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** What did I change from the above code? What is different about the output matrix? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 4\n",
    "poly = PolynomialFeatures(p, include_bias=False)\n",
    "X_powers = poly.fit_transform(X)\n",
    "\n",
    "X_powers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d229acf1",
   "metadata": {},
   "source": [
    "The trick in all this is that if I pass in this matrix to linear regression, the resulting model learned is exactly the model \n",
    "$$y = \\beta_0 + \\beta_1 X+ \\beta_2 X^2+ \\cdots+ \\beta_p X^p$$\n",
    "we wanted to use earlier so long as I line up the coefficients properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627fb98",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** For the original $X_{toy}$ data set, use all of the data to train a polynomial model with $p=3$ to predict $y$. What is the equation of the model learned, including all the values for the coefficients? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee408479",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Copy your code from above and modify it to use $k$-fold cross validation for $k=5$ to approximate the test error with a degree $p=3$ model. \n",
    "\n",
    "\n",
    "*Hint: You have easy-mode code from last class that involved the `cross_val_score` command that would be super useful here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b566a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9b1fa",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Using $k$-fold cross validation for $k=5$, set up code to approximate the test error for each of the polynomial models below. \n",
    "- $y = \\beta_0 + \\beta_1 X$\n",
    "- $y = \\beta_0 + \\beta_1 X + \\beta_2 X^2$\n",
    "- $y = \\beta_0 + \\beta_1 X+ \\beta_2 X^2+ \\beta_3 X^3$\n",
    "- $y = \\beta_0 + \\beta_1 X+ \\beta_2 X^2+ \\beta_3 X^3+ \\beta_4 X^4$\n",
    "- $y = \\beta_0 + \\beta_1 X+ \\beta_2 X^2+ \\beta_3 X^3+ \\beta_4 X^4+ \\beta_5 X^5$\n",
    "- $y = \\beta_0 + \\beta_1 X+ \\beta_2 X^2+ \\beta_3 X^3+ \\beta_4 X^4+ \\beta_5 X^5+ \\beta_6 X^6$\n",
    "\n",
    "Then plot your resulting test errors for each degree. What is the best choice of polynomial for this data set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4f3bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec458c7",
   "metadata": {},
   "source": [
    "If you still have some time, try the following:\n",
    "- see if you can figure out the test errors for everything through a degree 10 polynomial. \n",
    "- What happens to the graph if you mess around with the coefficients of the original polynomial that we used to generate the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f907668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f79113",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "-----\n",
    "### Congratulations, we're done!\n",
    "Written by Dr. Liz Munch, Michigan State University\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb8354f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3e3338d56a43a0108f5ff8ffc1915439f9812d920a0d5bf5d66e4a60c981234a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

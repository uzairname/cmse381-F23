{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfd68f5",
   "metadata": {},
   "source": [
    "# Lecture 20 - PCA and PCR\n",
    "## CMSE 381 - Fall 2023\n",
    "## Oct 27, 2023\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everyone's favorite standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# ML imports we've used previously\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7d5aa",
   "metadata": {},
   "source": [
    "# 1. PCA on Penguins\n",
    "![Palmer Penguins Picture](https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png)\n",
    "\n",
    "*Artwork by @allison_horst*\n",
    "\n",
    "\n",
    "For this lab, we are going to again use the <a href = \"https://allisonhorst.github.io/palmerpenguins/\">Palmer Penguins</a> data set by Allison Horst, Alison Hill, and Kristen Gorman. You should have done this in a previous notebook, but if you don't have the package installed to get the data, you can run \n",
    "```\n",
    "pip install palmerpenguins\n",
    "```\n",
    "to have access to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd59457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "penguins = load_penguins()\n",
    "penguins = penguins.dropna()\n",
    "\n",
    "#Shuffle the data\n",
    "penguins = penguins.sample(frac=1)\n",
    "penguins.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32626ac",
   "metadata": {},
   "source": [
    "Before we get to the full version, let's just take a look at two of the columns: flipper length and bill length. A nice thing we can do is to also color the data by which species label the data point has. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = penguins.bill_length_mm, \n",
    "                y = penguins.flipper_length_mm, \n",
    "                hue = penguins.species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53271b9",
   "metadata": {},
   "source": [
    "Before we get to it, we're going to just work with the columns that are numeric.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513fbbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_num = penguins.select_dtypes(np.number)\n",
    "penguins_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc78df3c",
   "metadata": {},
   "source": [
    "We will also use mean centered data to make the visualization easier (meaning shifting our data to have mean 0 in every column, and have standard deviation 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_normalized = (penguins_num - penguins_num.mean())/penguins_num.std()\n",
    "p_normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32336ff1",
   "metadata": {},
   "source": [
    "## PCA with just two input columns\n",
    "\n",
    "To try to draw pictures similar to what we just saw on the slides, we'll first focus on two of the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ab810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_subset2 = p_normalized[['bill_length_mm', 'flipper_length_mm']]\n",
    "penguins_subset2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ae24ad",
   "metadata": {},
   "source": [
    "We run PCA using the `PCA` command from `scikitlearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c282c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c8023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the PCA object\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit it using our data\n",
    "pca.fit(penguins_subset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588432ac",
   "metadata": {},
   "source": [
    "The `pca.components_` store information about the lines we are going to project our data onto. Specifically, each row gives us one of these lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22f7ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = penguins_subset2, \n",
    "                x = 'bill_length_mm', \n",
    "                y = 'flipper_length_mm', \n",
    "                hue = penguins.species)\n",
    "\n",
    "for i, comp in enumerate(pca.components_):\n",
    "    slope = comp[1]/comp[0]\n",
    "    plt.plot(np.array([-2,2]), slope*np.array([-2,2]))\n",
    "    \n",
    "plt.axis('square')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062b0cca",
   "metadata": {},
   "source": [
    "A common way to look at the relative importance of the PC's is to draw these components as vectors with length based on the explained variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc13489",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd942cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = penguins_subset2, \n",
    "                x = 'bill_length_mm', \n",
    "                y = 'flipper_length_mm', \n",
    "                hue = penguins.species)\n",
    "\n",
    "for i, (comp, var) in enumerate(zip(pca.components_, pca.explained_variance_)):\n",
    "    slope = comp[1]/comp[0]\n",
    "    plt.plot(np.array([-2,2]), slope*np.array([-2,2]))\n",
    "    \n",
    "    comp = comp * var  # scale component by its variance explanation power\n",
    "    plt.plot(\n",
    "        [0, comp[0]],\n",
    "        [0, comp[1]],\n",
    "        label=f\"Component {i}\",\n",
    "        linewidth=5,\n",
    "        color=f\"C{i + 2}\",\n",
    "    )\n",
    "\n",
    "plt.axis('square')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18823506",
   "metadata": {},
   "source": [
    "The next important part are the PC's, which we can get from the `pca` object as follows. I'm going to put them in a dataframe to make drawing and visualization easier. Basically, $PC_1$ is our $Z_1$ in the slides, and $PC_2$ is the $Z_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8899e455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transform function takes in bill,flipper data points, \n",
    "# and returns a PC1,PC2 coordinate for each one. \n",
    "penguins_pca = pca.transform(penguins_subset2)\n",
    "penguins_pca = pd.DataFrame(data = penguins_pca, columns = ['PC1', 'PC2'])\n",
    "penguins_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074ad72",
   "metadata": {},
   "source": [
    "This is the scatterplot of the data points transformed into the PC space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f8be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = penguins_pca, x = 'PC1', y = 'PC2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89914557",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** What are the PC coordinates for the first data point (index 0)?  Which quadrant would this point be drawn in? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132bd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45523adf",
   "metadata": {},
   "source": [
    "Below is code that emphasizes the projected points. \n",
    "\n",
    "&#9989; **<font color=red>Do this:</font>** the value of `index` below is just picking out a different point in our data set.  Mess around with this number. How do the X and star points move around as you change `index`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb442e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = penguins_subset2, \n",
    "                x = 'bill_length_mm', \n",
    "                y = 'flipper_length_mm', \n",
    "                hue = penguins.species)\n",
    "plt.axis('square')\n",
    "\n",
    "for i, (comp, var) in enumerate(zip(pca.components_, pca.explained_variance_)):\n",
    "    slope = comp[1]/comp[0]\n",
    "    plt.plot(np.array([-2,2]), slope*np.array([-2,2]))\n",
    "\n",
    "#===========\n",
    "# Emphasize one point and its projections\n",
    "#===========\n",
    "\n",
    "index = 180 #<---------- play with this!\n",
    "\n",
    "# Here's one data point\n",
    "plt.scatter([penguins_subset2.iloc[index,0]],\n",
    "            [penguins_subset2.iloc[index,1]], \n",
    "            marker = 'D', color = 'black', s = 150)\n",
    "\n",
    "# Here's the projection of that point on PC1 (X shape)\n",
    "plt.scatter([X1[index]], [Y1[index]], \n",
    "           marker = 'X', color = 'purple', s = 150)\n",
    "\n",
    "# And here's the projection of that point on PC2 (star)\n",
    "plt.scatter([X2[index]], [Y2[index]], \n",
    "           marker = '*', color = 'purple', s = 150)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6730ab0e",
   "metadata": {},
   "source": [
    "Everything we just did is great for understanding what the PCA is doing, but in reality, we're usually going to be looking at the data in the transformed space. \n",
    "\n",
    "&#9989; **<font color=red>Do this:</font>** Make a scatter plot of PC1 and PC2. Color the points by `penguins.species`. What do you notice about how the points have moved from the (`bill`, `flipper`) scatter plot? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535149ec",
   "metadata": {},
   "source": [
    "## Penguins PCA with all columns\n",
    "\n",
    "We used only two columns above for visualization, but we can instead use all the input columns to run our PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbbd8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "penguins_pca_all = pca.fit_transform(penguins_num)\n",
    "penguins_pca_all = pd.DataFrame(data = penguins_pca_all, \n",
    "                                columns = ['PC1', 'PC2', 'PC3', 'PC4'])\n",
    "penguins_pca_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f44c9",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Make a scatter plot of PC1 and PC2 using this new model, and again color the points by `penguins.species`. What do you notice about how the PC plot has changed from the previous setting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748cc09f",
   "metadata": {},
   "source": [
    "\n",
    "![Stop Icon](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Vienna_Convention_road_sign_B2a.svg/180px-Vienna_Convention_road_sign_B2a.svg.png)\n",
    "\n",
    "Great, you got to here! Hang out for a bit, there's more lecture before we go on to the next portion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450cfbaa",
   "metadata": {},
   "source": [
    "# PCR on Hitters Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9a257",
   "metadata": {},
   "source": [
    "# Loading in the data\n",
    "\n",
    "Ok, here we go, let's play with a baseball data set again. Note this cleanup is all the same as previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceeb83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../DataSets/Hitters.csv').dropna().drop('Player', axis = 1)\n",
    "df.info()\n",
    "dummies = pd.get_dummies(df[['League', 'Division', 'NewLeague']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Salary\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis = 1).astype('float64')\n",
    "\n",
    "# Define the feature set X.\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis = 1)\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b0c245",
   "metadata": {},
   "source": [
    "# Principal Component Regression \n",
    "\n",
    "Ok, let's take a hard left turn and go try out some of the dimension reduction methods from Section 6.3. `Scikit-learn` doesn't have a built in function to do PCR (aka PCA and then regression) but it's just as easy for us to do it ourselves. \n",
    "\n",
    "First step, let's figure out the `PCA` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd8765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale #<--- this does the scaling of variables for us\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faeaa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "print(X.shape)\n",
    "X_PC = pca.fit_transform(scale(X))\n",
    "print(X_PC.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ed73a",
   "metadata": {},
   "source": [
    "\"But Dr. Munch, you said PCA was supposed to do dimension reduction, why is my feature output the same size?\"\n",
    "\n",
    "Glad you asked, young data scientist. The PCA command outputs all of the PCs, all the way up through $p=19$ the original number of dimensions. \n",
    "\n",
    "So, if I want the first three PCs, I can get them out as follows. I'll put it in a data frame just to add column labels, but you don't need to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3c4249",
   "metadata": {},
   "outputs": [],
   "source": [
    "First3PCs = X_PC[:,:3]\n",
    "\n",
    "pd.DataFrame(First3PCs, columns = ['Z1','Z2', 'Z3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe7c33",
   "metadata": {},
   "source": [
    "Now we can just do regression on the PCs like before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_PC[:,:3], y)\n",
    "mean_squared_error(y,regr.predict(X_PC[:,:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24225df",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** My code above contains the rookie mistake of only reporting training error. Write modified code to return the 10-fold CV error of linear regression on the first 3 PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here #\n",
    "\n",
    "\n",
    "\n",
    "# You'll probably want this....\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f07704",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Take the code you figured out above to get the score for 10-fold CV on the first 3 PCs, and include it in the for-loop below to see how the MSE changes as the number of PCs you use changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X_PC)\n",
    "regr = LinearRegression()\n",
    "mse = []\n",
    "\n",
    "# Calculate MSE using CV for the 19 principle components, adding one component at a time.\n",
    "for i in np.arange(1, 20): # i is the number of principal components to use each time\n",
    "    # ====\n",
    "    score = 0 # Your code from above goes here!\n",
    "    # ====\n",
    "\n",
    "    mse.append(score)\n",
    "    \n",
    "# Plot results    \n",
    "plt.plot(mse, '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Predicting Salary')\n",
    "plt.xlim(xmin=-1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a735bf9",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Q:</font>** Based on the graph you generated above, how many PCs do you think you should use? \n",
    "\n",
    "*Note: Based on graphs I generated, I can see a few different options for what I might decide to use for number of principal components. This is one of those cases where you potentially have a different answer and/or reasoning from your neighbor, so I enourage you to talk this one through with your group.*\n",
    "\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d1d9fe",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Q:</font>** Of the models you've built so far (Ridge, Lasso, and PCR), which would you choose to use and why? \n",
    "\n",
    "*Note: This goes in the no-one-right-answer bucket. Go argue with your group.*\n",
    "\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f79113",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "-----\n",
    "### Congratulations, we're done!\n",
    "Written by Dr. Liz Munch, Michigan State University\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb8354f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b505362e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

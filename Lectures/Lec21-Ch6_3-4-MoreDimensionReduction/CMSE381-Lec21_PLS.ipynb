{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfd68f5",
   "metadata": {},
   "source": [
    "# Lecture 21 - PLS\n",
    "## CMSE 381 - Fall 2023\n",
    "## Oct 30, 2023\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everyone's favorite standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# ML imports we've used previously\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450cfbaa",
   "metadata": {},
   "source": [
    "# PLS on Hitters Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9a257",
   "metadata": {},
   "source": [
    "# Loading in the data\n",
    "\n",
    "Ok, here we go, let's play with a baseball data set again. Note this cleanup is all the same as the last lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceeb83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../DataSets/Hitters.csv').dropna().drop('Player', axis = 1)\n",
    "df.info()\n",
    "dummies = pd.get_dummies(df[['League', 'Division', 'NewLeague']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672d6f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = df.Salary\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis = 1).astype('float64')\n",
    "\n",
    "# Define the feature set X.\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis = 1)\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012e8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here we have the normalized data.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_normalized = StandardScaler().fit_transform(X)\n",
    "X_normalized = pd.DataFrame(X_normalized, columns = X.columns)\n",
    "X_normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a30fe2",
   "metadata": {},
   "source": [
    "# Principal Least Squares (PLS)\n",
    "\n",
    "The command do do PLS in `Scikit-learn` is  `PLSRegression`. Below is a quick code that runs PLS on our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4dc711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ddffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pls = PLSRegression(n_components=3)\n",
    "pls.fit(X_normalized,y)\n",
    "yhat = pls.predict(X_normalized)\n",
    "mean_squared_error(y,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69b7075",
   "metadata": {},
   "source": [
    "But like last time, we can also use the `cross_val_score` function to get the CV score easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b3c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pls = PLSRegression(n_components=3)\n",
    "scores = cross_val_score(pls, X_normalized, y, cv=10, scoring='neg_mean_squared_error')\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb760f",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>**  Like last time, your job is to test a PLS model for an increasing number of components used. I recommend using the `cross_val_score` with `scoring='neg_mean_squared_error'`. What number of components would you use? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e81b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X_normalized)\n",
    "mse = []\n",
    "\n",
    "# Calculate MSE using CV for an increasing number of components, \n",
    "# adding one component at a time.\n",
    "for i in np.arange(1, 20): # i is the number of components to use each time\n",
    "    # ====\n",
    "    score = 0 # Your code to figure out the score each time goes in here. \n",
    "    # ====\n",
    "\n",
    "    mse.append(score)\n",
    "    \n",
    "# Plot results    \n",
    "plt.plot(mse, '-v')\n",
    "plt.xlabel('Number of  components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Predicting Salary')\n",
    "plt.xlim(xmin=-1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d355614d",
   "metadata": {},
   "source": [
    "## GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716579ac",
   "metadata": {},
   "source": [
    "Let's make our lives a little easier! We keep doing $k$-fold CV over lots of parameters, here's a command that we can use to do what we did above in fewer lines. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49da92b0",
   "metadata": {},
   "source": [
    "First, I'm going to use `Pipeline` to build up a list of things I want to do for my data. Here, I'm going to do the PCR system we used last time (In a little bit you're going to update all this to do PLS for you). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c776aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of PCA and linear regression\n",
    "pca = PCA(n_components =2)\n",
    "linreg = LinearRegression ()\n",
    "\n",
    "# Buid the pipeline and give each thing in the pipeline a name\n",
    "pipe = Pipeline ([('pca', pca), ('linreg', linreg)])\n",
    "\n",
    "# Do the usual fitting with our input data\n",
    "pipe.fit(X_normalized, y)\n",
    "\n",
    "# Pull out whatever stuff from the specific step I'm interested in\n",
    "pipe.named_steps['linreg'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f18083f",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>**  How do you get the principal components used in the PCA step? \n",
    "\n",
    "*Hint: They're stored in the PCA step as `components_`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac523ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879bdd1b",
   "metadata": {},
   "source": [
    "Now what we can do is work with a grid of inputs we want to search. You can be all kinds of fancy and change more than one input, but we're only ever doing one for this class. \n",
    "\n",
    "So in my case, what I want to do is mess around with the number of components passed into PCA by setting this from 1 to 19.  Notice that because of my pipeline step, the key for the entry in the dictionary for `param_grid` has `pca` first since that's the part of the pipeline I want, then two underscores, then the name of the input for `pca` that I'm messing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208901c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's me creating my parameter grid\n",
    "param_grid = {'pca__n_components': range (1, 20)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb21f425",
   "metadata": {},
   "source": [
    "Now I get to pass this into the `GridSearchCV` command, which does exacly what you did above. It takes everything in the defined pipeline, does "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96bd292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b488c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This actually does the fit\n",
    "gridPCA = GridSearchCV(pipe , param_grid, cv=kf_10 ,scoring='neg_mean_squared_error')\n",
    "gridPCA.fit(X_normalized, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0020d",
   "metadata": {},
   "source": [
    "Now I want to find out what it figured out. \n",
    "\n",
    "Here's how I can find the mean test score over all the entries in the parameter grid. The negative is because internally, sklearn uses negative MSE. Note that there are entries corresponding to $[1,\\cdots,19]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4510a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "-gridPCA.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c1110",
   "metadata": {},
   "source": [
    "And now I can plot to see what's up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = param_grid['pca__n_components']\n",
    "\n",
    "plt.plot(n_comp , -gridPCA.cv_results_['mean_test_score'], label = 'PCR')\n",
    "plt.legend()\n",
    "plt.ylabel('Cross -validated MSE')\n",
    "plt.xlabel('# principal components')\n",
    "plt.xticks(n_comp [::2])\n",
    "plt.ylim ([100000 ,140000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4a1ee",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>**  Do the same thing but for the PLS pipeline discussed above. \n",
    "- I recommend changing my named `gridPCA` to something like `gridPLS`. \n",
    "- You actually don't need the `Pipeline` here since you're only doing `PLSRegression` so the code should actually be simpler. \n",
    "- Draw the resulting plot with the PCR and PLS drawn on top of each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12050f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f79113",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "-----\n",
    "### Congratulations, we're done!\n",
    "Written by Dr. Liz Munch, Michigan State University\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc92824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
